{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce05e6b",
   "metadata": {},
   "source": [
    "Part #1 - Some Global Functions\n",
    "\n",
    "Function A - Importing Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5216f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = Name of the excel file\n",
    "# s = Sheet Name within the excel file\n",
    "# r = Range of data which needs to be imported\n",
    "# d = Data object that needs to be sent while exporting back\n",
    "\n",
    "def import_excel(n,s,r):\n",
    "    import xlwings as xw\n",
    "    wb = xw.Book(n)\n",
    "    sheet = wb.sheets[s]\n",
    "    data = sheet.range(r).value\n",
    "    return data \n",
    "\n",
    "def export_excel(n,s,r,d):\n",
    "    import xlwings as xw\n",
    "    wb = xw.Book(n)\n",
    "    sheet = wb.sheets[s]\n",
    "    sheet.range(r).value = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112349f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = import_excel('Market Data.xlsx','USDIRS','A3:B12')\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b91517",
   "metadata": {},
   "source": [
    "Function B - MFBD Holiday Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2f55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt = Date on which operation is to be performed\n",
    "#cal1 = Country of Calendar 1, For USA pass argument US\n",
    "#cal2 = Country of Calendar 2, For India pass argument IN .. This is an optional argument\n",
    "# At present this function suports a max of 2 calendars only for MFBD\n",
    "\n",
    "def mfbd(dt,cal1, cal2=\"NIL\"):\n",
    "    from datetime import date\n",
    "    from datetime import timedelta\n",
    "    import holidays\n",
    "    calendar1 = holidays.country_holidays(cal1)\n",
    "    if cal2 == \"NIL\":\n",
    "        while dt.weekday() > 4 or dt in calendar1:\n",
    "            dt = dt + timedelta(days=1)\n",
    "    else:\n",
    "        calendar2 = holidays.country_holidays(cal2)\n",
    "        while dt.weekday() > 4 or dt in calendar1 or dt in calendar2:\n",
    "            dt = dt + timedelta(days=1)\n",
    "    #dt = date(2025,5,24)\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e488e4d",
   "metadata": {},
   "source": [
    "Function C - Curve Bootstrapping Function\n",
    "\n",
    "Gives you a set of Discount Factors and Zero Rates for a given curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data,i=-1,p=1):\n",
    "    from datetime import date,timedelta\n",
    "    import holidays\n",
    "    import pandas as pd\n",
    "    #PricingDate = date.today() #Use todays date as the Pricing Date\n",
    "    PricingDate = date(2025,5,19) # For testing currently\n",
    "    SDate = mfbd(PricingDate + timedelta(days=2),'US') #Calculating the Spot Start Date\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['Year','Rate']\n",
    "    i = int(i)\n",
    "    if i>-1:\n",
    "        df.iat[i,1]=df.iat[i,1]+0.0001*p\n",
    "        \n",
    "    df['MatDate'] = SDate \n",
    "    df['PmtDate'] = SDate\n",
    "\n",
    "    #Populate Maturity Date and Payment Date adjusting for pay delay\n",
    "\n",
    "    for index,row in df.iterrows():\n",
    "        df.at[index,'MatDate'] = mfbd(row['MatDate'] + pd.offsets.DateOffset(years=row['Year']),'US')\n",
    "        df.at[index,'PmtDate'] = mfbd(df.at[index,'MatDate'] + pd.offsets.DateOffset(days=2),'US') #SOFR Swaps have 2 days Payment Delay\n",
    "    \n",
    "    df['MatDate'] = pd.to_datetime(df['MatDate']).dt.date # To remove hh:mm:ss from Date\n",
    "    df['PmtDate'] = pd.to_datetime(df['PmtDate']).dt.date\n",
    "    df['DC'] = df['PmtDate'].diff(periods=1) #Compute Periodic day counts\n",
    "    df.at[0,'DC']= pd.to_timedelta(df.at[0,'PmtDate']-SDate) #Compute first DC value from Start Date\n",
    "    df['DC'] = pd.to_numeric(df['DC'].dt.days)\n",
    "    df['DC']=df['DC']/360 #Since Daycount is Act/360 for USD SOFR\n",
    " \n",
    "    import numpy as np\n",
    "    df['DF'] = 1.000000\n",
    " \n",
    "    SDate = mfbd(SDate,'US')\n",
    "\n",
    "    #Just testing, delete later\n",
    "    for index,row in df.iterrows():\n",
    "        df.at[index,'DF'] = 1/(pow(1+row['Rate'],row['Year']))\n",
    "\n",
    "\n",
    "    #Compute actual Curve DFs for USD SOFR\n",
    "    #The for loop is massively simplified to solve a linear system of equations. Will write a full document on how this is calculated later. For queries in the interim email pushkargondane@gmail.com \n",
    "    dfdccumprodsum = 0\n",
    "    for index,row in df.iterrows():\n",
    "        df.at[index,'DF'] = (1 - df.at[index,'Rate']*dfdccumprodsum)/(1+df.at[index,'Rate']*df.at[index,'DC'])\n",
    "        dfdccumprodsum = dfdccumprodsum + df.at[index,'DC']*df.at[index,'DF']\n",
    "\n",
    "    #Compute Zeros for USD SOFR\n",
    "    # Please note that zeros requires an assumption of the approach. Here we are going to try and match\n",
    "    # it with that of bloomberg by assuming continuous compounding\n",
    "    # Different systems implement this bit in a different manner\n",
    "\n",
    "\n",
    "    df['Zero']=1.000000\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        p = pd.to_timedelta(df.at[index,'PmtDate']-df.at[index,'MatDate'])\n",
    "        t = float(p.days)/365 + df.at[index,'Year']\n",
    "        #print(t)\n",
    "        df.at[index,'Zero'] = np.log(1/df.at[index,'DF'])/t\n",
    "        \n",
    "\n",
    "\n",
    "    #print(df)\n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a6fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year      Rate     MatDate     PmtDate        DC        DF      Zero\n",
      "0   1.0  0.040750  2026-05-21  2026-05-26  1.027778  0.959802  0.040474\n",
      "1   2.0  0.037767  2027-05-21  2027-05-24  1.008333  0.927426  0.037517\n",
      "2   3.0  0.037051  2028-05-22  2028-05-24  1.016667  0.895086  0.036878\n",
      "3   4.0  0.037070  2029-05-21  2029-05-23  1.011111  0.862696  0.036873\n",
      "4   5.0  0.037380  2030-05-21  2030-05-23  1.013889  0.830086  0.037204\n",
      "5   6.0  0.037867  2031-05-21  2031-05-23  1.013889  0.797265  0.037727\n",
      "6   7.0  0.038378  2032-05-21  2032-05-24  1.019444  0.764618  0.038295\n",
      "7   8.0  0.038862  2033-05-23  2033-05-25  1.016667  0.732701  0.038851\n",
      "8   9.0  0.039313  2034-05-22  2034-05-24  1.011111  0.701706  0.039336\n",
      "9  10.0  0.039746  2035-05-21  2035-05-23  1.011111  0.671430  0.039813\n"
     ]
    }
   ],
   "source": [
    "test=bootstrap(data)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253de67c",
   "metadata": {},
   "source": [
    "Function D - Validate DF vs Alternate System like Bloomberg\n",
    "\n",
    "![title][def]\n",
    "\n",
    "[def]: SOFRBBG.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdd969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(df):\n",
    "    import pandas as pd\n",
    "    bbgdata = import_excel('Market Data.xlsx','USDIRS','g3:h12')\n",
    "    bbg = pd.DataFrame(bbgdata)\n",
    "    bbg.columns=['BBGDF','BBGZero']\n",
    "    comp = df[['Year','MatDate','PmtDate','DF','Zero']]\n",
    "    comp['BBGDF'] = bbg['BBGDF'] #Need to find a better method to append columns as this is giving a warning \n",
    "    comp['BBGZero'] = bbg['BBGZero']\n",
    "    comp['DF Gap(bps)']=(comp['BBGDF']-comp['DF'])*10000/comp['BBGDF']\n",
    "    comp['Zero Gap(bps)']=(comp['BBGZero']-comp['Zero'])*10000/comp['BBGZero']\n",
    "    print(comp)\n",
    "    comp=comp.drop(columns=['Year','BBGDF','BBGZero','DF Gap(bps)','Zero Gap(bps)'])\n",
    "    #Send the data back to excel \n",
    "    export_excel('Market Data.xlsx','USDIRS','L2:L11',comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a95d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year     MatDate     PmtDate        DF      Zero     BBGDF   BBGZero  \\\n",
      "0   1.0  2026-05-21  2026-05-26  0.959802  0.040474  0.960094  0.040502   \n",
      "1   2.0  2027-05-21  2027-05-24  0.927426  0.037517  0.927482  0.037538   \n",
      "2   3.0  2028-05-22  2028-05-24  0.895086  0.036878  0.895045  0.036826   \n",
      "3   4.0  2029-05-21  2029-05-23  0.862696  0.036873  0.862658  0.036859   \n",
      "4   5.0  2030-05-21  2030-05-23  0.830086  0.037204  0.830051  0.037193   \n",
      "5   6.0  2031-05-21  2031-05-23  0.797265  0.037727  0.797234  0.037716   \n",
      "6   7.0  2032-05-21  2032-05-24  0.764618  0.038295  0.764668  0.038271   \n",
      "7   8.0  2033-05-23  2033-05-25  0.732701  0.038851  0.732677  0.038802   \n",
      "8   9.0  2034-05-22  2034-05-24  0.701706  0.039336  0.701686  0.039303   \n",
      "9  10.0  2035-05-21  2035-05-23  0.671430  0.039813  0.671413  0.039794   \n",
      "\n",
      "   DF Gap(bps)  Zero Gap(bps)  \n",
      "0     3.043136       6.936892  \n",
      "1     0.600978       5.616469  \n",
      "2    -0.453521     -14.162227  \n",
      "3    -0.444027      -3.806096  \n",
      "4    -0.424608      -3.162720  \n",
      "5    -0.395006      -2.811359  \n",
      "6     0.658414      -6.361351  \n",
      "7    -0.323512     -12.637953  \n",
      "8    -0.288918      -8.292641  \n",
      "9    -0.248494      -4.845787  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/4br4skc15dlb750j33xn06rh0000gn/T/ipykernel_1179/832407605.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comp['BBGDF'] = bbg['BBGDF'] #Need to find a better method to append columns as this is giving a warning\n"
     ]
    }
   ],
   "source": [
    "compare(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf05052",
   "metadata": {},
   "source": [
    "Note that while the gap in DFs is miniscule (less than 1 bps vs BBG DFs) the zeros are slightly more different. They dont impact any calculations as the approach to calculate zeros simply depends on the base formula used for zeros. \n",
    "\n",
    "Valuation differences may arise if you choose to interpolate on zeros vs interpolate on DFs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3421d",
   "metadata": {},
   "source": [
    "Function E - Pricing an Interest Rate Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63291349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priceIRS(i=-1,p=1):\n",
    "    #Part A - Easy - Price the Fixed Leg of the IRS\n",
    "    import pandas as pd\n",
    "    from datetime import date,timedelta\n",
    "    import holidays\n",
    "\n",
    "    data = import_excel('Market Data.xlsx','USDIRS','A3:B12') #Import market data\n",
    "    df = bootstrap(data,i,p) #Bootstrap the curve\n",
    "    PricingDate = date(2025,5,19) # For testing currently\n",
    "    SDate = mfbd(PricingDate + timedelta(days=2),'US') #Calculating the Spot Start Date\n",
    "    #Add 1 as Discount Factor for Spot Date in the Discount Factors Data Frame\n",
    "    df.loc[-1] = [0,0,SDate,SDate,0,1,0]\n",
    "    df.index = df.index+1\n",
    "    df = df.sort_index()\n",
    "\n",
    "    fixed = import_excel('TradePrice.xlsx','SOFR','b3:g12')\n",
    "    fixed = pd.DataFrame(fixed)\n",
    "    fixed.columns=['SDate','EDate','PDate','Notional','Amort','FixRate']\n",
    "\n",
    "    #Apply MFBD Calendar on the dates\n",
    "    for index,row in fixed.iterrows():\n",
    "        fixed.at[index,'SDate'] = mfbd(row['SDate'],'US')\n",
    "        fixed.at[index,'EDate'] = mfbd(row['EDate'],'US')\n",
    "        fixed.at[index,'PDate'] = mfbd(row['PDate'],'US') \n",
    "\n",
    "    fixed['DC']=fixed['EDate']-fixed['SDate']\n",
    "    fixed['DC']=pd.to_timedelta(fixed['DC']).dt.days.astype(float)/360\n",
    "    fixed['CF']=fixed['Notional']*fixed['FixRate']*fixed['DC']\n",
    "\n",
    "    #Interpolate Discount Factors from Bootstrapping procedure\n",
    "\n",
    "\n",
    "    df[\"PmtDate\"] = df[\"PmtDate\"].astype(\"datetime64[ns]\")\n",
    "    df['PmtDate']=df['PmtDate'].values.astype(float)\n",
    "    df[\"MatDate\"] = df[\"MatDate\"].astype(\"datetime64[ns]\")\n",
    "    df['MatDate']=df['MatDate'].values.astype(float)\n",
    "\n",
    "    import numpy as np\n",
    "    fixed['DF']=np.interp(fixed['PDate'],df['PmtDate'],df['DF']) #Simple linear interpolation \n",
    "    fixed['NPV'] = fixed['DF']*fixed['CF']\n",
    "\n",
    "    floating = import_excel('TradePrice.xlsx','SOFR','b16:i25')\n",
    "    floating = pd.DataFrame(floating)\n",
    "    floating.columns=['SDate','EDate','PDate','Notional','Amort','Index','Leverage','Spread']\n",
    "\n",
    "\n",
    "    #Apply MFBD Calendar on the dates\n",
    "    for index,row in floating.iterrows():\n",
    "        floating.at[index,'SDate'] = mfbd(row['SDate'],'US')\n",
    "        floating.at[index,'EDate'] = mfbd(row['EDate'],'US')\n",
    "        floating.at[index,'PDate'] = mfbd(row['PDate'],'US')#Assumed standard Pay Delay of 2 Days\n",
    "\n",
    "    floating['DC']=floating['EDate']-floating['SDate']\n",
    "    floating['DC']=pd.to_timedelta(floating['DC']).dt.days.astype(float)/360\n",
    "    floating['DF']=np.interp(floating['PDate'],df['PmtDate'],df['DF']) #Simple linear interpolation \n",
    "    floating['DFSdate']=np.interp(floating['SDate'],df['MatDate'],df['DF']) #Simple linear interpolation \n",
    "    floating['DFEdate']=np.interp(floating['EDate'],df['MatDate'],df['DF']) #Simple linear interpolation \n",
    "    floating['Forecast']=((floating['DFSdate']/floating['DFEdate'])-1)/floating['DC']\n",
    "\n",
    "    #The first forecast rate is incorrect as it ignores the impact due to the \n",
    "    # daily fixings which have already occured. We need to make an adjustment for that\n",
    "    fixings = import_excel('Market Data.xlsx','SOFRFix','C2:D348') #Import market data\n",
    "    fixings = pd.DataFrame(fixings)\n",
    "    fixings.columns=['Date','FixRate']\n",
    "\n",
    "    filter=floating.iat[0,0] #Obtain the start date of the fixings\n",
    "    fixings = fixings[fixings['Date']>=filter] #Note we have assumed that fixings data is latest available. \n",
    "\n",
    "    #We have also assumed that the fix for the 2 unknown days between last fixing and spot is also available\n",
    "    #You can alternatively assume its the same as last fix and wont make much difference to overall calculations\n",
    "\n",
    "    fixings['DC'] = fixings['Date'].diff(periods=1) #Compute Periodic day counts\n",
    "    fixings.iat[0,2] = fixings.iat[0,0] - mfbd(fixings.iat[0,0]+timedelta(days=1),'US')\n",
    "    fixings['DC']=fixings['DC']*-1\n",
    "    fixings['Days']=fixings['DC']/timedelta(days=1)/360 #Using Act/360 Day count convention\n",
    "    fixings['FixDays']=fixings['FixRate']*fixings['Days']/100\n",
    "\n",
    "    floating.iat[0,12] = floating.iat[0,12] + fixings['FixDays'].sum()\n",
    "    floating['NPV']=floating['Notional']*(floating['Leverage']*floating['Forecast']+floating['Spread'])*floating['DC']*floating['DF']\n",
    "\n",
    "\n",
    "    return fixed['NPV'].sum()+floating['NPV'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedfe6f",
   "metadata": {},
   "source": [
    "Lets now generate the bucketed sensivitiy / DV01 for this trade\n",
    "\n",
    "Approach #1 - Simpler which involves shifting the underlying curve up by 1 bps only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ad3b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current NPV :  436822.06\n",
      "Bucket  1 : 6.31\n",
      "Bucket  2 : -8.53\n",
      "Bucket  3 : -11.15\n",
      "Bucket  4 : -16.54\n",
      "Bucket  5 : -19.22\n",
      "Bucket  6 : -23.43\n",
      "Bucket  7 : -31.58\n",
      "Bucket  8 : -33.03\n",
      "Bucket  9 : -2769.42\n",
      "Bucket  10 : -5318.7\n",
      "Total Dv01 :  -8225.31\n"
     ]
    }
   ],
   "source": [
    "NetPV = priceIRS() #Compute the unperturbed PV first\n",
    "print (\"Current NPV : \", round(NetPV,2))\n",
    "Dv01 = 0\n",
    "for i in range(10):\n",
    "    NewPV = priceIRS(i)\n",
    "    print(\"Bucket \",i+1 ,\":\" , round(NewPV - NetPV,2))\n",
    "\n",
    "    Dv01 = Dv01 + NewPV - NetPV\n",
    "print (\"Total Dv01 : \", round(Dv01,2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a2385",
   "metadata": {},
   "source": [
    "Approach #2 - Slightly more computationally intensive and involves shifting the underlying curve up and below by 1 bps and then taking their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91d0857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current NPV :  436822.06\n",
      "Bucket  1 : 6.31\n",
      "Bucket  2 : -8.53\n",
      "Bucket  3 : -11.15\n",
      "Bucket  4 : -16.54\n",
      "Bucket  5 : -19.23\n",
      "Bucket  6 : -23.43\n",
      "Bucket  7 : -31.56\n",
      "Bucket  8 : -33.03\n",
      "Bucket  9 : -2769.69\n",
      "Bucket  10 : -5319.22\n",
      "Total Dv01 :  -8226.09\n"
     ]
    }
   ],
   "source": [
    "NetPV = priceIRS() #Compute the unperturbed PV first\n",
    "print (\"Current NPV : \", round(NetPV,2))\n",
    "Dv01 = 0\n",
    "for i in range(10):\n",
    "    NewPVup = priceIRS(i,1) \n",
    "    NewPVdn = priceIRS(i,-1)\n",
    "    print(\"Bucket \",i+1 ,\":\" , round((NewPVup - NewPVdn)/2,2))\n",
    "\n",
    "    Dv01 = Dv01 + (NewPVup - NewPVdn)/2\n",
    "print (\"Total Dv01 : \", round(Dv01,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517442de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
